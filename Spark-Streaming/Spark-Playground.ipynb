{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"tocheading\">Table of Contents</b>\n",
    "<div id=\"toc\"></div>\n",
    "<script type=\"text/javascript\" src=\"https://raw.github.com/kmahelona/ipython_notebook_goodies/master/ipython_notebook_toc.js\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Spark from http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz\n",
    "\n",
    "* Spark version 1.6.0\n",
    "* Prebuilt for Hadoop 2.6\n",
    "\n",
    "\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. *findspark* [1][2] does the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/soumendra/bin/spark-1.6.0-bin-hadoop2.6\")\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"helloworld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's test that our setup is working by counting the number of lines in a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('/home/soumendra/RecommenderSystem')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up *findspark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the automatically detected location, call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark-1.6.0-bin-hadoop2.6'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark can add a startup file to the current IPython profile so that the enviornment vaiables will be properly set and pyspark will be imported upon IPython startup. This file is created when *edit_profile* is set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ipython --profile=myprofile\n",
    "findspark.init(\"/home/soumendra/bin/spark-1.6.0-bin-hadoop2.6\", edit_profile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark can also add to the .bashrc configuration file if it is present so that the enviornment variables will be properly set whenever a new shell is opened. This is enabled by setting the optional argument *edit_rc* to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init(\"/home/soumendra/bin/spark-1.6.0-bin-hadoop2.6\", edit_rc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up with *docker*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jupyter/docker-stacks/tree/master/pyspark-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "* [1] Discussion in Jupyter GitHub repo: https://github.com/jupyter/notebook/issues/309#issuecomment-134540424\n",
    "* [2] *findspark* at github: https://github.com/minrk/findspark\n",
    "* [3] \n",
    "* [4] \n",
    "* [5] \n",
    "* [6] \n",
    "* [7] \n",
    "* [8] \n",
    "* [9] \n",
    "* [10] \n",
    "* [11] \n",
    "* [12] \n",
    "* [13] \n",
    "* [14] \n",
    "* [15] \n",
    "* [16] \n",
    "* [17] \n",
    "* [18] \n",
    "* [19] \n",
    "* [20] \n",
    "* [21] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
